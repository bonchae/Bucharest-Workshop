{"cells":[{"cell_type":"markdown","source":["# Kansas State University"],"metadata":{"id":"vtxsLN3xX80y"},"id":"vtxsLN3xX80y"},{"cell_type":"code","source":["from IPython.display import YouTubeVideo\n","video_id = \"Zs-q1Rl3FmQ\"\n","YouTubeVideo(video_id, width=800, height=500, start=113)"],"metadata":{"id":"LPp3isM8YEA-"},"id":"LPp3isM8YEA-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import YouTubeVideo\n","video_id = \"9GPaBj76j38\"\n","YouTubeVideo(video_id, width=800, height=500)"],"metadata":{"id":"k4eKhniIYYqp"},"id":"k4eKhniIYYqp","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"e837db37-90c4-46b4-bded-365b6a8355da","metadata":{"id":"e837db37-90c4-46b4-bded-365b6a8355da"},"source":["# Test Your AI Knowledge!\n","\n","> ## Test Your Deep Learning (DL) Knowledge!\n","\n",">> Go to [DL Matchmaker](https://ml-matchmaker-damku2db2w6pk8lqb7wyf4.streamlit.app/)\n","\n","> ## Test your Machine Learning (ML) Knowledge\n","\n",">> Go to [ML Matchmaker](https://ml-matchmaker-damku2db2w6pk8lqb7wyf4.streamlit.app/)"]},{"cell_type":"markdown","id":"846d4346-a849-4e3c-86bc-f3f98e456a04","metadata":{"id":"846d4346-a849-4e3c-86bc-f3f98e456a04"},"source":["# Introduction to Machine Learning for Business Applications"]},{"cell_type":"markdown","id":"ed96a72a-1146-4863-a567-52f14d6a7c4e","metadata":{"id":"ed96a72a-1146-4863-a567-52f14d6a7c4e"},"source":["In this session, we'll learn:\n","- What AI, ML, and DL are (and how they differ)\n","- Key types of ML and DL with business examples\n","- Code demos of ML in action using Python"]},{"cell_type":"markdown","id":"4d829762-9f5a-4c40-8e73-c641b2a536d5","metadata":{"id":"4d829762-9f5a-4c40-8e73-c641b2a536d5"},"source":["## AI vs ML vs DL\n","Let's understand the relationships between Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL).\n","\n","- **AI**: Any technique that enables machines to mimic human behavior.\n","- **ML**: Subset of AI that enables machines to improve from data.\n","- **DL**: Subset of ML using neural networks with multiple layers.\n"]},{"cell_type":"markdown","id":"1f824f5c-09e0-47b0-85f8-031afc8493ab","metadata":{"id":"1f824f5c-09e0-47b0-85f8-031afc8493ab"},"source":["<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/AIvsML.png\">\n","\n","[Source](https://www.simplilearn.com/ice9/free_resources_article_thumb/AIvsML.png)"]},{"cell_type":"markdown","source":["### Examples of Deep Learning\n","\n","<img src=\"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/cecbccba-6358-476e-9fd8-e2807de9f220/Frame_118.png?t=1693044751\" width=500>\n","\n","[Go to Hugging Face](https://huggingface.co/) and explore [the pre-trained models available on the website](https://huggingface.co/models) and [The AI App Directory](https://huggingface.co/spaces)"],"metadata":{"id":"QlXQq90_0PRQ"},"id":"QlXQq90_0PRQ"},{"cell_type":"markdown","source":["#### > [Generate Your Favoriate Image](https://huggingface.co/spaces/bytedance-research/UNO-FLUX)"],"metadata":{"id":"_3ndpEh3v_OH"},"id":"_3ndpEh3v_OH"},{"cell_type":"markdown","source":["#### > [Turn Your Idea Into An App](https://huggingface.co/spaces/osanseviero/InstantCoder)"],"metadata":{"id":"rwR3Otvpv_Q9"},"id":"rwR3Otvpv_Q9"},{"cell_type":"markdown","source":["#### > [Your AI Agent for Academic Research](https://huggingface.co/spaces/ginipick/AgentX-Papers)"],"metadata":{"id":"_Y4DNsxtv_LX"},"id":"_Y4DNsxtv_LX"},{"cell_type":"markdown","source":["### Examples of Machine Learning"],"metadata":{"id":"ouWyOPv73Gj3"},"id":"ouWyOPv73Gj3"},{"cell_type":"markdown","source":["| United States | Bucharest |\n","|:--------------|:----------|\n","| [Zillow](https://www.zillow.com/) | [Imobiliare.ro](https://www.imobiliare.ro) |\n","| [Realtor.com](https://www.realtor.com/) | [Storia.ro](https://www.storia.ro) |\n","| [Redfin](https://www.redfin.com/) | [OLX.ro (Real Estate)](https://www.olx.ro/imobiliare/) |\n"],"metadata":{"id":"Ap7VOjL347FV"},"id":"Ap7VOjL347FV"},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","\n","# 1. Create a Bucharest housing data\n","data = pd.DataFrame({\n","    'size_sqm': [48, 55, 70, 85, 100, 60, 75, 90],\n","    'price_eur': [72000, 82500, 126000, 144500, 180000, 135000, 168000, 225000],\n","    'building_type': ['old', 'old', 'old', 'old', 'old', 'new', 'new', 'new']\n","})\n","print(data)\n","\n","# 2. Convert 'building_type' into numeric (old=0, new=1)\n","data['building_type_num'] = data['building_type'].map({'old': 0, 'new': 1})\n","\n","# 3. Train a Linear Regression model with two features\n","X = data[['size_sqm', 'building_type_num']]\n","y = data['price_eur']\n","\n","# Step 1: Initialize the algorithm\n","model =\n","\n","# Step 2: This is where the Magic happens!\n","model.\n","\n","# 4. Print the equation\n","slope_size = model.coef_[0]\n","slope_building = model.coef_[1]\n","intercept = model.intercept_\n","print(\"----------------------------------------------------------------------------------\")\n","print(f\"Equation: price = {slope_size:.2f} * size_sqm + {slope_building:.2f} * building_type_num + {intercept:.2f}\")"],"metadata":{"id":"Er1eo-XIxuOe"},"id":"Er1eo-XIxuOe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5. Predict price for a 70 sqm new apartment (building_type = 1)\n","predicted_price = model.predict(pd.DataFrame({'size_sqm': [70], 'building_type_num': [1]}))\n","print(f\"Predicted price for 70 sqm **new** apartment: €{predicted_price[0]:.0f}\")"],"metadata":{"id":"401pgU840cKk"},"id":"401pgU840cKk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Examples of AI that are not ML or DL:\n","\n","- Medical diagnosis\n","- Loan approval systems using hand-coded rules\n","- **Rule-based** systems\n"],"metadata":{"id":"USaxbylusZhN"},"id":"USaxbylusZhN"},{"cell_type":"code","source":["# your financial information :)\n","\n","credit_score = 700\n","income = 50000\n","debt_to_income_ratio = 0.4"],"metadata":{"id":"VGGIiN61y_Um"},"id":"VGGIiN61y_Um","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# implement a simple expert system (Loan approval)\n","\n","if credit_score > 700 and income > 50000 and debt_to_income_ratio < 0.4:\n","    approve_loan =\n","else:\n","    approve_loan =\n","\n","\n"],"metadata":{"id":"2ArhJfBnzFBh"},"id":"2ArhJfBnzFBh","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"AbFlS-IXm84R","metadata":{"id":"AbFlS-IXm84R"},"source":["## The Evolution of AI"]},{"cell_type":"markdown","id":"OOlW6Em_m6ku","metadata":{"id":"OOlW6Em_m6ku"},"source":["<img src=\"https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs13174-018-0087-2/MediaObjects/13174_2018_87_Fig5_HTML.png?as=webp\">\n","\n","[Source](https://jisajournal.springeropen.com/articles/10.1186/s13174-018-0087-2/figures/5)"]},{"cell_type":"markdown","id":"x2vbAmVh6HTw","metadata":{"id":"x2vbAmVh6HTw"},"source":["<img src=\"https://www.researchgate.net/publication/384694535/figure/fig3/AS:11431281282385423@1728324583591/The-timeline-of-the-development-of-LLMs-from-2018-to-2024-June-showcasing-key.ppm\">\n","\n","[Source](https://www.researchgate.net/figure/The-timeline-of-the-development-of-LLMs-from-2018-to-2024-June-showcasing-key_fig3_384694535)"]},{"cell_type":"markdown","id":"gbk0EJXdUn7j","metadata":{"id":"gbk0EJXdUn7j"},"source":["# Machine Learning vs Deep Learning\n","\n","| **Machine Learning** | **Deep Learning** |\n","|----------------------|-------------------|\n","| A subset of AI | A subset of machine learning |\n","| Can train on smaller data sets | Requires **large amounts of data** |\n","| Requires more human intervention to correct and learn | Learns on its own from environment and past mistakes |\n","| Shorter training and lower accuracy | **Longer training** and higher accuracy |\n","| Makes simple, linear correlations | Makes **non-linear, complex correlations** |\n","| Can train on a **CPU** (central processing unit) | Needs a specialized **GPU (graphics processing unit)** to train |\n","| Business applications:<br>**• Credit scoring**<br>**• Fraud detection**<br>**• Customer churn prediction**<br>**• Recommendation systems**<br>**• Price optimization** | Business applications:<br>**• Natural language processing**<br>**• Computer vision (image recognition)**<br>**• Autonomous vehicles**<br>**• Advanced chatbots**<br>**• Medical diagnosis from imaging**<br>**• Speech recognition** |"]},{"cell_type":"markdown","id":"9bzBCdKPB1eF","metadata":{"id":"9bzBCdKPB1eF"},"source":["<img src=\"https://cdn.shopify.com/s/files/1/0560/4789/4710/t/20/assets/cpu_vs_gpu_parallel_processing-HHfd0J.True?v=1707822664\">\n","\n"]},{"cell_type":"code","execution_count":null,"id":"eEO3mAN8zaCg","metadata":{"id":"eEO3mAN8zaCg"},"outputs":[],"source":["from IPython.display import YouTubeVideo\n","\n","# Embed the video\n","YouTubeVideo(\"-P28LKWTzrI\", width=800, height=450)"]},{"cell_type":"markdown","id":"Hc9wFO97UzSM","metadata":{"id":"Hc9wFO97UzSM"},"source":["## The Performance of ML vs DL Models"]},{"cell_type":"markdown","id":"eaaff7b6-d507-4a6e-808e-a8266c0ed28b","metadata":{"id":"eaaff7b6-d507-4a6e-808e-a8266c0ed28b"},"source":["<center><img src=\"https://www.researchgate.net/profile/Junaid-Qadir/publication/348703101/figure/fig1/AS:983057658023936@1611390618125/The-accuracy-of-ML-approaches-compared-to-the-accuracy-of-DL-models-with-respect-to-data.ppm\" width=500></center>\n","\n","[Source](https://www.researchgate.net/profile/Junaid-Qadir/publication/348703101/figure/fig1/AS:983057658023936@1611390618125/The-accuracy-of-ML-approaches-compared-to-the-accuracy-of-DL-models-with-respect-to-data.ppm)"]},{"cell_type":"markdown","source":["# Deep Learning"],"metadata":{"id":"k5L_yk9d6UDh"},"id":"k5L_yk9d6UDh"},{"cell_type":"markdown","source":["<center><img src=\"https://towardsdatascience.com/wp-content/uploads/2021/12/1hkYlTODpjJgo32DoCOWN5w.png\" width=700></center>\n","\n","<center><img src=\"https://miro.medium.com/v2/format:webp/1*Ne7jPeR6Vrl1f9d7pLLG8Q.jpeg\" width=500></center>\n","\n","[Source](https://medium.com/@b.terryjack/introduction-to-deep-learning-feed-forward-neural-networks-ffnns-a-k-a-c688d83a309d)"],"metadata":{"id":"eRC9U1xn6p3c"},"id":"eRC9U1xn6p3c"},{"cell_type":"markdown","source":["```python\n","model = Sequential()\n","model.add(Input(shape=(X_train_scaled.shape[1],)))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(16, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n"],"metadata":{"id":"lfKtbp-69qmt"},"id":"lfKtbp-69qmt"},{"cell_type":"markdown","source":["## What is “Deep”?\n","\n","“Deep” just means **many layers** of neurons. Each layer **extracts increasingly complex features** from data (e.g., images, texts).\n","\n","<img src=\"https://i0.wp.com/developersbreach.com/wp-content/uploads/2020/08/cnn_banner.png?fit=1400%2C658&ssl=1\" width=700>"],"metadata":{"id":"bZtfrJvH64Hh"},"id":"bZtfrJvH64Hh"},{"cell_type":"markdown","source":["```python\n","model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(28, 28, 1)),                    # Input: grayscale image\n","    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'), # Conv layer to extract patterns\n","    tf.keras.layers.MaxPooling2D(2, 2),                    # Downsample by 2\n","    tf.keras.layers.Flatten(),                             # Flatten 2D → 1D\n","    tf.keras.layers.Dense(64, activation='relu'),          # Hidden layer\n","    tf.keras.layers.Dense(10, activation='softmax')        # Output: 10 class probabilities\n","])\n"],"metadata":{"id":"dqBjHz6f-Vc9"},"id":"dqBjHz6f-Vc9"},{"cell_type":"markdown","source":["## Popularity Ranking of DL Architectures (as of 2025)\n","\n","| Rank | Architecture     | Popularity               | Primary Use Cases                                        |\n","|------|------------------|--------------------------|----------------------------------------------------------|\n","| 1️⃣   | Transformers      | ⭐⭐⭐⭐⭐ *(most popular)*    | LLMs, NLP, vision, audio, multimodal                     |\n","| 2️⃣   | CNNs              | ⭐⭐⭐⭐                     | Image classification, object detection, vision tasks     |\n","| 3️⃣   | GANs              | ⭐⭐⭐                      | Image generation, style transfer, data augmentation      |\n","| 4️⃣   | RNNs / LSTMs      | ⭐⭐                       | Legacy NLP, time series prediction, audio modeling       |\n"],"metadata":{"id":"Qq35jAFs6eO6"},"id":"Qq35jAFs6eO6"},{"cell_type":"markdown","source":["<h2 style=\"text-align: center;\">Deep Learning Algorithm Hierarchy with Business Applications</h2>\n","\n","<table border=\"1\" cellspacing=\"0\" cellpadding=\"8\" style=\"margin: auto; text-align: left; font-family: sans-serif;\">\n","  <tr style=\"background-color: #8cbeb2; text-align: center;\">\n","    <th><strong>Type</strong></th>\n","    <th><strong>Business Applications</strong></th>\n","    <th><strong>Real-World Examples</strong></th>\n","  </tr>\n","\n","  <tr>\n","    <td><strong>CNNs</strong><br><small>Convolutional Neural Networks</small></td>\n","    <td>\n","      <ul>\n","        <li><strong>Image classification (e.g., product recognition)</strong></li>\n","        <li>Medical image diagnostics</li>\n","        <li>Quality control in manufacturing</li>\n","        <li>Facial recognition in security</li>\n","      </ul>\n","    </td>\n","    <td>\n","      <ul>\n","        <li>Google Lens</li>\n","        <li>Apple Face ID</li>\n","        <li>Siemens AI Medical Imaging</li>\n","        <li>Amazon Go Surveillance</li>\n","      </ul>\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><strong>RNNs</strong><br><small>Recurrent Neural Networks</small></td>\n","    <td>\n","      <ul>\n","        <li>Stock and sales forecasting</li>\n","        <li>Customer churn prediction</li>\n","        <li>Speech-to-text transcription</li>\n","        <li>Chatbot conversation modeling</li>\n","      </ul>\n","    </td>\n","    <td>\n","      <ul>\n","        <li>Google Voice</li>\n","        <li>Nuance Dragon Speech</li>\n","        <li>Apple Siri (early versions)</li>\n","        <li>Amazon Forecast</li>\n","      </ul>\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><strong>GANs</strong><br><small>Generative Adversarial Networks</small></td>\n","    <td>\n","      <ul>\n","        <li>Synthetic data generation</li>\n","        <li>Product design and prototyping</li>\n","        <li>AI-generated media and marketing visuals</li>\n","        <li>Art and creative content generation</li>\n","      </ul>\n","    </td>\n","    <td>\n","      <ul>\n","        <li><strong>DALL·E</strong> (OpenAI)</li>\n","        <li>Midjourney</li>\n","        <li>StyleGAN (NVIDIA)</li>\n","        <li>Runway ML</li>\n","      </ul>\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><strong>Transformers</strong><br><small>LLMs / Attention-Based Models</small></td>\n","    <td>\n","      <ul>\n","        <li>Customer support chatbots</li>\n","        <li>Sentiment analysis and social listening</li>\n","        <li>Text summarization and document search</li>\n","        <li>Personalized recommendations and Q&A</li>\n","      </ul>\n","    </td>\n","    <td>\n","      <ul>\n","        <li><strong>ChatGPT</strong> (OpenAI)</li>\n","        <li>Google Bard</li>\n","        <li>GitHub Copilot</li>\n","        <li>Google Translate</li>\n","      </ul>\n","    </td>\n","  </tr>\n","\n","  <tr>\n","    <td><strong>DRL</strong><br><small>Deep Reinforcement Learning</small></td>\n","    <td>\n","      <ul>\n","        <li>Autonomous vehicles and robotics</li>\n","        <li>Dynamic pricing systems</li>\n","        <li>Supply chain and inventory optimization</li>\n","        <li>Simulated training environments</li>\n","      </ul>\n","    </td>\n","    <td>\n","      <ul>\n","        <li>Waymo Self-Driving Cars</li>\n","        <li>DeepMind AlphaGo</li>\n","        <li>OpenAI Five (Dota 2)</li>\n","        <li>Amazon Warehouse Optimization</li>\n","      </ul>\n","    </td>\n","  </tr>\n","</table>\n"],"metadata":{"id":"3YLLtZxK7IqM"},"id":"3YLLtZxK7IqM"},{"cell_type":"markdown","source":["## Two Broad Ways to Use Deep Learning Models\n","\n","### 1. Using Pre-trained Models\n","\n","You take a model that has already been trained on a large dataset (like **ImageNet**, **COCO**, or a large text corpus).\n","\n","You can:\n","- Use it **as-is** for inference\n","- **Fine-tune** it on your own smaller dataset\n","- **Pros**: Faster, requires less data and compute, great for transfer learning\n","\n","### 2. Training Models from Scratch\n","\n","You define your model architecture and train it from the beginning using your dataset.\n","\n","- **Pros**: Complete control, better for novel or domain-specific problems  \n","- **Cons**: Requires large datasets, longer training time, and more compute resources"],"metadata":{"id":"o8WVJGHo7V9E"},"id":"o8WVJGHo7V9E"},{"cell_type":"markdown","source":["# Examples: Deep Learning"],"metadata":{"id":"q8pW97NB7uFL"},"id":"q8pW97NB7uFL"},{"cell_type":"markdown","source":["## Text Data"],"metadata":{"id":"0qyfO46Q_sqM"},"id":"0qyfO46Q_sqM"},{"cell_type":"markdown","source":["### Emotion Detection"],"metadata":{"id":"0IORm7FxAtE1"},"id":"0IORm7FxAtE1"},{"cell_type":"markdown","source":["#### Pre-trained Models\n","Use case: Go beyond \"positive/negative\" — detect emotions like joy, anger, sadness"],"metadata":{"id":"-beDxDSI7wHK"},"id":"-beDxDSI7wHK"},{"cell_type":"markdown","source":["<img src=\"https://webflow-amber-prod.gumlet.io/620e4101b2ce12a1a6bff0e8/66ab6846124b51c486c24b3e_640f1bb03074900cbf0f28f3_What-are-the-Ivy-League-schools.webp\" width=500>\n","\n","**\"I can't believe I got in! I'm so happy and feel very grateful.\"**"],"metadata":{"id":"krIhj6Ht71Se"},"id":"krIhj6Ht71Se"},{"cell_type":"code","source":["from transformers import pipeline\n","\n","emotion = pipeline(\"text-classification\",\n","                   model=\"j-hartmann/emotion-english-distilroberta-base\",\n","                   top_k=None)\n","\n","results = emotion(\"I can't believe I got in! I'm so happy and feel very grateful.\")\n","\n","for row in results:\n","    for item in row:\n","        print(f\"{item['label']:<10} -> {item['score']:.4f}\")"],"metadata":{"id":"Tc5DSiab6VkL"},"id":"Tc5DSiab6VkL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try a different sentence\n","\n","\n"],"metadata":{"id":"CClZEiy46VnK"},"id":"CClZEiy46VnK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Training Emotion Detection Model from Scratch\n","\n","<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3ltsv1uzGR6UBjZ6CUs04A.jpeg\" width=500>"],"metadata":{"id":"8yzfTSR78Iqg"},"id":"8yzfTSR78Iqg"},{"cell_type":"markdown","source":["```python\n","model.add(Embedding(input_dim=5000, output_dim=128))  # Maps each word (integer) to a 128-dimensional vector\n","model.add(LSTM(64))                                   # Adds an LSTM layer with 64 units to capture sequential patterns\n","model.add(Dense(1, activation='sigmoid'))             # Adds a Dense output layer with sigmoid activation for binary classification\n"],"metadata":{"id":"DvbK0HIz-Snr"},"id":"DvbK0HIz-Snr"},{"cell_type":"code","source":["import pandas as pd\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","data = {\n","    \"text\": [\n","        \"I'm so happy and grateful\",\n","        \"This is terrible. I'm really mad\",\n","        \"I feel scared and anxious\",\n","        \"What a surprise! I didn’t expect that\",\n","        \"I'm feeling so loved and supported\",\n","        \"This makes me really sad\"\n","    ],\n","    \"emotion\": [\n","        \"joy\",\n","        \"anger\",\n","        \"fear\",\n","        \"surprise\",\n","        \"love\",\n","        \"sadness\"\n","    ]\n","}\n","\n","df = pd.DataFrame(data)\n","df"],"metadata":{"id":"kS9KSubU6VrK"},"id":"kS9KSubU6VrK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['text'])\n","X = tokenizer.texts_to_sequences(df['text'])\n","X = pad_sequences(X, padding='post')\n","\n","# Encode labels\n","le = LabelEncoder()\n","y = le.fit_transform(df['emotion'])\n","\n","# Check vocab size\n","vocab_size = len(tokenizer.word_index) + 1"],"metadata":{"id":"XXxFcFDk6VuE"},"id":"XXxFcFDk6VuE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 1: Declare the architecture."],"metadata":{"id":"FgLa2PGs-_rf"},"id":"FgLa2PGs-_rf"},{"cell_type":"code","source":["model = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=16, input_length=X.shape[1]),\n","    LSTM(16),\n","    Dense(6, activation='')  # 6 emotions\n","])\n","\n","model.compile(optimizer='',\n","              loss='',\n","              metrics=[''])\n","model.summary()"],"metadata":{"id":"FxeNFxl56Vw9"},"id":"FxeNFxl56Vw9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["| Aspect                     | binary_crossentropy                          | sparse_categorical_crossentropy                  |\n","|-----------------------------|----------------------------------------------|--------------------------------------------------|\n","| Use Case                   | Binary classification (yes/no)              | Multiclass classification (3+ classes, labels as integers) |\n","| Output Layer               | Sigmoid (1 neuron)                           | Softmax (one neuron per class)                   |\n","| Label Format               | 0 or 1 (binary labels)                       | Integer labels like 0, 1, 2, 3, etc.              |\n","| Example                    | Spam or not spam                             | Classifying cats, dogs, horses (labels 0, 1, 2)   |\n","| Special Note               | Only for two classes                        | For multiple classes without one-hot encoding    |\n"],"metadata":{"id":"q149lPWiCMW7"},"id":"q149lPWiCMW7"},{"cell_type":"markdown","source":["Step 2: Let the magic come true!"],"metadata":{"id":"W3jxW0AS_EfE"},"id":"W3jxW0AS_EfE"},{"cell_type":"code","source":["model.fit( ,  , epochs=30, verbose=1)"],"metadata":{"id":"9F4bICsI8luO"},"id":"9F4bICsI8luO","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Use the trained model for prediction"],"metadata":{"id":"KIgRnE2o_LVt"},"id":"KIgRnE2o_LVt"},{"cell_type":"code","source":["# Your test sentence\n","test_text = [\"I can't believe I got in! I'm so happy and feel very grateful.\"]\n","\n","# Preprocess the test input\n","test_seq = tokenizer.texts_to_sequences(test_text)\n","test_pad = pad_sequences(test_seq, maxlen=X.shape[1], padding='post')\n","\n","# Predict emotion\n","pred = model.predict(test_pad)\n","emotion_label = le.inverse_transform([pred.argmax()])[0]\n","\n","# Print Output\n","print(f\"Text: {test_text[0]}\")\n","print(f\"Predicted Emotion: {emotion_label}\")"],"metadata":{"id":"wjJ8bDuG8pmg"},"id":"wjJ8bDuG8pmg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["I just uploaded the trained model to my Huggingface account: https://huggingface.co/kevinbschae\n","\n","Now, this model is available to the world! People can use our model as a pre-trained model for **emotion detection** :)"],"metadata":{"id":"pH-avEft8yO3"},"id":"pH-avEft8yO3"},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from tensorflow import keras\n","import pickle\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Load model from HuggingFace\n","model_path = hf_hub_download(\"kevinbschae/emotion-model\", filename=\"emotion_model.keras\")\n","model = keras.models.load_model(model_path)\n","\n","# Load tokenizer\n","tokenizer_path = hf_hub_download(\"kevinbschae/emotion-model\", filename=\"tokenizer.pkl\")\n","with open(tokenizer_path, \"rb\") as f:\n","    tokenizer = pickle.load(f)\n","\n","# Load label encoder\n","encoder_path = hf_hub_download(\"kevinbschae/emotion-model\", filename=\"label_encoder.pkl\")\n","with open(encoder_path, \"rb\") as f:\n","    le = pickle.load(f)\n","\n","model.summary()"],"metadata":{"id":"OH_6CHcT9Mkf"},"id":"OH_6CHcT9Mkf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test input\n","test_text = [\"I can't believe I got in! I'm so happy and feel very grateful.\"]\n","\n","# Tokenize and pad\n","sequence = tokenizer.texts_to_sequences(test_text)\n","padded = pad_sequences(sequence, maxlen=model.input_shape[1], padding='post')  # use same padding as training\n","\n","# Predict\n","pred_probs = model.predict(padded)\n","pred_class = np.argmax(pred_probs, axis=1)\n","\n","# Decode to emotion label\n","pred_emotion = le.inverse_transform(pred_class)\n","\n","print(\"Predicted emotion:\", pred_emotion[0])"],"metadata":{"id":"n2C9HKqw9MnA"},"id":"n2C9HKqw9MnA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Translation"],"metadata":{"id":"ReP5DPLFEAfA"},"id":"ReP5DPLFEAfA"},{"cell_type":"code","source":["translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ro\")\n","text = \"I Love You All!\"\n","\n","translation = translator(text)[0]['translation_text']\n","print(translation)"],"metadata":{"id":"eG7Vw8GM9MqA"},"id":"eG7Vw8GM9MqA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Audio Transcription (Speech-to-Text)\n","Use case: Convert speech into text using Whisper.\n","\n","[openai/whisper-small](https://huggingface.co/openai/whisper-small) is a pre-trained model for automatic speech recognition (ASR) and speech translation."],"metadata":{"id":"0NGpQs47EN7q"},"id":"0NGpQs47EN7q"},{"cell_type":"code","source":["import textwrap # print output in multiple lines"],"metadata":{"id":"X9bI2J2InwX6"},"id":"X9bI2J2InwX6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Audio, display\n","\n","# Direct link to the audio file\n","audio_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n","\n","# Embed audio player\n","display(Audio(audio_url))"],"metadata":{"id":"xeZTfLA3ERW_"},"id":"xeZTfLA3ERW_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["asr = pipeline(\n","    \"automatic-speech-recognition\",\n","    model=\"openai/whisper-small\",\n","    generate_kwargs={\"task\": \"translate\", \"language\": \"en\"}\n",")\n","\n","output = asr(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n","\n","wrapped_text = textwrap.fill(output[\"text\"], width=80)\n","\n","print(\"Transcription:\\n\")\n","print(wrapped_text)"],"metadata":{"id":"1cWFuakwERa_"},"id":"1cWFuakwERa_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Text Generation / Chatbots: Use case: Writing, storytelling, character dialogue"],"metadata":{"id":"lkM7o6tEDrwx"},"id":"lkM7o6tEDrwx"},{"cell_type":"code","source":["from transformers import pipeline\n","import torch\n","\n","# Use GPU if available\n","device = 0 if torch.cuda.is_available() else -1\n","\n","# Create text generation pipeline (defaults to GPT-2)\n","generator = pipeline(\"text-generation\", model=\"gpt2\", device=device)\n","\n","# Generate text\n","output = generator(\n","    \"Once upon a time in Bucharest ...\",\n","    max_length=500,\n","    truncation=True,\n","    pad_token_id=generator.tokenizer.eos_token_id\n",")\n","\n","wrapped = textwrap.fill(output[0][\"generated_text\"], width=80)\n","print(\"\\n Generated Text:\\n\" + \"-\"*80)\n","print(wrapped)\n","print(\"-\"*80)"],"metadata":{"id":"DjE27m8BDu56"},"id":"DjE27m8BDu56","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try to generate different texts\n","\n","\n"],"metadata":{"id":"WyZXZyNSDu9S"},"id":"WyZXZyNSDu9S","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Data"],"metadata":{"id":"bcDP8YnZA6H3"},"id":"bcDP8YnZA6H3"},{"cell_type":"markdown","source":["<img src=\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg?crop=1.00xw:0.756xh;0,0.134xh&resize=1024:\" width=400>"],"metadata":{"id":"Oi31PYHKBC3K"},"id":"Oi31PYHKBC3K"},{"cell_type":"markdown","source":["### [ResNet-50 v1.5](https://huggingface.co/microsoft/resnet-50) (CNN model)\n","\n","\"ResNet (Residual Network) is a convolutional neural network. ResNet model pre-trained on ImageNet-1k at resolution 224x224.\"\n","\n","- 1,000 object categories (classes)\n","- 1.2 million training images\n","- 50,000 validation images"],"metadata":{"id":"2oOjU3djBQBv"},"id":"2oOjU3djBQBv"},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from transformers import pipeline\n","import torch\n","\n","import textwrap # print output in multiple lines"],"metadata":{"id":"vJY_uUNpA-e0"},"id":"vJY_uUNpA-e0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for CUDA (GPU)\n","device = 0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n","print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n","\n","# Load image classification pipeline with ResNet-50 (CNN)\n","classifier = pipeline(\n","    \"image-classification\",\n","    model=\"microsoft/resnet-50\",\n","    device=device,\n","    use_fast=True  # Use the fast image processor to avoid the warning\n",")\n","\n","# Classify the image\n","result = classifier(\"https://hips.hearstapps.com/hmg-prod/images/pembroke-welsh-corgi-royalty-free-image-1726720011.jpg\")\n","\n","for item in result:\n","    print(f\"Label: {item['label']}, Score: {item['score']:.4f}\")"],"metadata":{"id":"y0CZU1GFA-pc"},"id":"y0CZU1GFA-pc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try the above image. what labels do you expect?\n","# https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\n","\n","\n","\n"],"metadata":{"id":"XYjxi4jCA-sz"},"id":"XYjxi4jCA-sz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### [CLIP model](https://huggingface.co/docs/transformers/en/model_doc/clip) (Transformer model)\n","\n","\"CLIP is a is a multimodal vision and language model motivated by **overcoming the fixed number of object categories** when training a computer vision model. CLIP learns about images directly from raw text by jointly training on 400M (image, text) pairs. Pretraining on this scale enables **zero-shot transfer** to downstream tasks.\" Developed by the OpenAI organization.\n"],"metadata":{"id":"EIbM44WwBiYb"},"id":"EIbM44WwBiYb"},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch, requests\n","\n","# Load model & processor\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"openai/clip-vit-base-patch32\"\n","model = CLIPModel.from_pretrained(model_id).to(device)\n","processor = CLIPProcessor.from_pretrained(model_id)\n","\n","# Image and candidate captions\n","image = Image.open(requests.get(\n","    \"https://people.com/thmb/TlNhUj4fJ8pnJNpEvUN-015Jcac=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():focal(979x595:981x597):format(webp)/bts-members-1-03a9c478f1794c448bcb5f74bf94812c.jpg\",\n","    stream=True).raw)\n","\n","texts = [\"a photo of BTS\",\n","         \"a photo of a dog\",\n","         \"a photo of a band\",\n","         \"a group of men\"]\n","\n","# Predict\n","inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True).to(device)\n","probs = model(**inputs).logits_per_image.softmax(dim=1)[0]\n","\n","# Display results\n","print(\"\\n CLIP Similarity Scores:\")\n","for text, p in zip(texts, probs):\n","    print(f\"{text:<25} -> {p:.4f}\")"],"metadata":{"id":"2-IaMinVA-vi"},"id":"2-IaMinVA-vi","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://s.yimg.com/ny/api/res/1.2/UrUx_Vbbk413oGzvWSklPA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTI0MDA7aD0xNjAw/https://media.zenfs.com/en/parade_250/0b28a903a2ed548d063f996165786cd4\" width=500>"],"metadata":{"id":"6pWDSHzbB4f1"},"id":"6pWDSHzbB4f1"},{"cell_type":"code","source":["# Try the above image. Who is this person?\n","# \"https://s.yimg.com/ny/api/res/1.2/UrUx_Vbbk413oGzvWSklPA--/YXBwaWQ9aGlnaGxhbmRlcjt3PTI0MDA7aD0xNjAw/https://media.zenfs.com/en/parade_250/0b28a903a2ed548d063f996165786cd4\"\n","\n","\n","\n"],"metadata":{"id":"yqbQhYbYA-yL"},"id":"yqbQhYbYA-yL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GANs: Generative Adversarial Networks: Image generations (DALL·E (OpenAI))"],"metadata":{"id":"mBaXRw7GFS8B"},"id":"mBaXRw7GFS8B"},{"cell_type":"markdown","source":["### Stable Diffusion 2.1 Version on Huggingface"],"metadata":{"id":"_72an1TwFcXI"},"id":"_72an1TwFcXI"},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import torch\n","from PIL import Image\n","from IPython.display import display\n","\n","# Check if CUDA is available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Load the model (use full ID: CompVis/stable-diffusion-v1-4)\n","pipe = StableDiffusionPipeline.from_pretrained(\n","    \"CompVis/stable-diffusion-v1-4\",\n","    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n","    low_cpu_mem_usage=True  # helps on CPU!\n",").to(device)\n","\n","# Prompt\n","prompt = \"A futuristic smart home device in minimal style product photography\"\n","\n","# Generate\n","print(\"Generating image... please wait ⏳\")\n","image = pipe(prompt).images[0]\n","\n","# Display\n","display(image)"],"metadata":{"id":"Zy-dV6pXFUOy"},"id":"Zy-dV6pXFUOy","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}