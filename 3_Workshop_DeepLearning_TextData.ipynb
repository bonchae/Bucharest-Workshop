{"cells":[{"cell_type":"markdown","source":["# NLP for Business Applications: Sentiment Analysis\n","\n","1. Text Representation Methods (Three)\n","\n","2. Pre-trained models\n","\n","3. Models from scratch"],"metadata":{"id":"xciMm92WvT70"},"id":"xciMm92WvT70"},{"cell_type":"markdown","source":["**Choosing the Right Deep Learning Model**\n","\n","| Task                                | Use FFNN? | Use LSTM/RNN? | Use CNN? | Use Transformer? |\n","|-------------------------------------|-----------|---------------|----------|------------------|\n","| Customer churn (structured data)    | ✅ Yes    | ❌            | ❌       | ⚠️ Emerging use  |\n","| Product review sentiment (text)     | ❌        | ✅ Yes        | ❌       | ✅ Yes           |\n","| Sales forecasting (time series)     | ❌        | ✅            | ❌       | ✅ Growing trend |\n","| Image classification                | ❌        | ❌            | ✅ Yes   | ✅ (Vision Transformer) |\n"],"metadata":{"id":"n2eRsFDbvvbF"},"id":"n2eRsFDbvvbF"},{"cell_type":"markdown","source":["# 1. Text Representation: Old & New\n","\n","- Bag of Words (BoW)\n","- TF-IDF (Term Frequency-Inverse Document Frequency)\n","- **Word Embedding**: Words are learned as dense vectors that **capture semantic relationships** (e.g., ```king - man + woman = queen```)\n","\n","These different text representations become the input for (supervised) text models such as sentiment analysis."],"metadata":{"id":"fj6MHcDyblR2"},"id":"fj6MHcDyblR2"},{"cell_type":"markdown","source":["## Bag of Words (BoW)\n","\n","This example shows how **one-hot encoding** assigns each word a unique vector (**BoW**). However, there are some key limitations:\n","\n","- There’s **no relationship** between words like `'buy'` and `'product'` — they are treated as completely unrelated.\n","- All vectors are the **same length as the vocabulary**, which can become very large.\n","- The vectors are **sparse** — most of the elements are zeros, which is inefficient for computation.\n"],"metadata":{"id":"mmuY_UsAdWJ3"},"id":"mmuY_UsAdWJ3"},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"id":"JJNZ7p3A0vpn"},"id":"JJNZ7p3A0vpn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Toy customer reviews\n","docs = [\n","    \"Great product and fast delivery\",\n","    \"Terrible product, very slow service\",\n","    \"Fast shipping and excellent service\",\n","]"],"metadata":{"id":"SiKIXbB2cjxr"},"id":"SiKIXbB2cjxr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Token-level one-hot using CountVectorizer with binary encoding\n","vectorizer = CountVectorizer(binary=True)  # binary=True makes it one-hot like\n","X = vectorizer.fit_transform(docs)\n","\n","# Convert to DataFrame for visualization\n","one_hot_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n","display(one_hot_df)"],"metadata":{"id":"tlDRub-t01ww"},"id":"tlDRub-t01ww","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## the TF-IDF Matrix\n","\n","- Each **row** represents a document (in this case, a customer review).\n","- Each **column** represents a word from the vocabulary.\n","- Each **value** in the matrix reflects how important that word is in the context of the specific document — higher means more important.\n","- **Common but uninformative words** (like *and*, *the*, *is*) are automatically down-weighted.\n"],"metadata":{"id":"spZ_LSXNcScN"},"id":"spZ_LSXNcScN"},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","\n","# Toy customer reviews\n","docs = [\n","    \"Great product and fast delivery\",\n","    \"Terrible product, very slow service\",\n","    \"Fast shipping and excellent service\",\n","]\n","\n","# TF-IDF vectorizer\n","vectorizer = TfidfVectorizer()\n","X_tfidf = vectorizer.fit_transform(docs)\n","\n","# Convert to DataFrame for readability\n","tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n","display(tfidf_df)"],"metadata":{"id":"Lemqq_-JboFt"},"id":"Lemqq_-JboFt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Word Embeddings (GloVe Example)\n","\n","- Word embeddings represent words as **dense vectors** in a high-dimensional space (e.g., 50 dimensions).\n","- Unlike one-hot or TF-IDF, embeddings **capture semantic meaning**.\n","- Words like `'buy'` and `'purchase'` have **similar vector representations**, so they appear close together in the embedding space.\n","- Pre-trained models like **GloVe** or **Word2Vec** are trained on massive corpora (e.g., Wikipedia, news) and can be used directly for downstream NLP tasks.\n"],"metadata":{"id":"jJOxLvEjdqnK"},"id":"jJOxLvEjdqnK"},{"cell_type":"code","source":["!pip install --upgrade gensim --quiet"],"metadata":{"id":"2Y1CQtFudhGR"},"id":"2Y1CQtFudhGR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load small pre-trained word embeddings (GloVe 50-dimensions)\n","glove = api.load(\"glove-wiki-gigaword-50\")     # Trained on Wikipedia + Gigaword news corpus"],"metadata":{"id":"LTMd_N-ufhcR"},"id":"LTMd_N-ufhcR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at the vector for a word\n","print(\"Vector for 'buy':\")\n","print(glove['buy'])"],"metadata":{"id":"kOWgkfva19_w"},"id":"kOWgkfva19_w","execution_count":null,"outputs":[]},{"cell_type":"code","source":["glove.most_similar(\"buy\")"],"metadata":{"id":"NGj-qzj32Ghw"},"id":"NGj-qzj32Ghw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try a different word (e.g., king)\n"],"metadata":{"id":"jWAhvmWsv26-"},"id":"jWAhvmWsv26-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Similarity between two words\n","print(\"\\nSimilarity between 'buy' and 'purchase':\")\n","print(glove.similarity('buy', 'purchase'))"],"metadata":{"id":"F_WbteQM2An4"},"id":"F_WbteQM2An4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# try another two words (e.g., king, queen)\n","\n"],"metadata":{"id":"v6Fc7LDawAvP"},"id":"v6Fc7LDawAvP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Most similar words to 'cheap'\n","print(\"\\nWords most similar to 'cheap':\")\n","print(glove.most_similar('cheap'))"],"metadata":{"id":"CyLfiTwI2Ark"},"id":"CyLfiTwI2Ark","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Word vector analogy\n","result = glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])  # glove[\"king\"] - glove[\"man\"] + glove[\"woman\"]\n","print(result[:5])  # Top 5 most similar words to the analogy"],"metadata":{"id":"e7B1wh4Wqt0a"},"id":"e7B1wh4Wqt0a","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"c068f766","metadata":{"id":"c068f766"},"source":["# 2. Sentiment Analysis Using Pre-trained Models"]},{"cell_type":"markdown","source":["<img src=\"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/cecbccba-6358-476e-9fd8-e2807de9f220/Frame_118.png?t=1693044751\" width=500>\n","\n","Founded in 2016\n","\n","Thousands of models (e.g., BERT, ChatGPT) you can use **without training from scratch**!\n","\n","[Go to Hugging Face](https://huggingface.co/) and explore [the pre-trained models available on the website](https://huggingface.co/models)."],"metadata":{"id":"4bSoXbPI_c0z"},"id":"4bSoXbPI_c0z"},{"cell_type":"code","execution_count":null,"id":"0a10be16","metadata":{"id":"0a10be16"},"outputs":[],"source":["# Sample product reviews\n","reviews = [\n","    \"The product quality is excellent and exceeded my expectations!\",\n","    \"Terrible experience. I want a refund.\",\n","    \"Pretty good, but the delivery was slow.\",\n","    \"Absolutely love it! Will buy again.\",\n","    \"The item broke after one week. Very disappointed.\",\n","]\n","\n","df = pd.DataFrame({'Review': reviews})\n","df"]},{"cell_type":"code","execution_count":null,"id":"d7b15e5c","metadata":{"id":"d7b15e5c"},"outputs":[],"source":["from transformers import pipeline"]},{"cell_type":"code","execution_count":null,"id":"1a33e1c6","metadata":{"id":"1a33e1c6"},"outputs":[],"source":["# Use HuggingFace sentiment analysis pipeline\n","classifier = pipeline(\"sentiment-analysis\",\n","                      model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n","results = classifier(df['Review'].tolist())\n","\n","for result in results:\n","    print(f\"Label: {result['label']}, with score: {round(result['score'], 4)}\")"]},{"cell_type":"code","source":["# Add results to DataFrame\n","df['Sentiment'] = [r['label'] for r in results]\n","df['Score'] = [r['score'] for r in results]\n","df"],"metadata":{"id":"7ZMZdVYp1w7f"},"id":"7ZMZdVYp1w7f","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"e83dc926","metadata":{"id":"e83dc926"},"outputs":[],"source":["sentiment_counts = df['Sentiment'].value_counts()\n","sentiment_counts.plot(kind='bar', color=['lightgreen', 'salmon'], edgecolor='black')\n","plt.title('Sentiment Breakdown')\n","plt.ylabel('Count')\n","plt.xlabel('Sentiment')\n","plt.xticks(rotation=0)\n","plt.show()\n"]},{"cell_type":"markdown","source":["## Search for sentiment analysis models"],"metadata":{"id":"mn1Wn8qQ33hW"},"id":"mn1Wn8qQ33hW"},{"cell_type":"code","source":["# Install huggingface_hub if not already installed\n","!pip -q install -q huggingface_hub"],"metadata":{"id":"3S5et9Fb4gKq"},"id":"3S5et9Fb4gKq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","# Initialize the API\n","api = HfApi()\n","\n","# Search for sentiment analysis models\n","models = api.list_models(search=\"sentiment\",\n","                         task=\"sentiment-analysis\", limit=50)\n","\n","# Print the models\n","for model in models:\n","    print(model.id)"],"metadata":{"id":"UZV5_bbj3ikc"},"id":"UZV5_bbj3ikc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Twitter-roBERTa-base for Sentiment Analysis\n","\n","This is a [RoBERTa-base model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark."],"metadata":{"id":"q7bPXbw05BrC"},"id":"q7bPXbw05BrC"},{"cell_type":"code","source":["model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","sentiment_task = pipeline(\"sentiment-analysis\", model=model_name)\n","sentiment_task(\"Covid cases are increasing fast!\")"],"metadata":{"id":"hKbffd2L5A9n"},"id":"hKbffd2L5A9n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Financial sentiment analysis: FinancialBERT for Sentiment Analysis\n","- [FinancialBERT](https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis) is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model.\n","\n","- The model was fine-tuned for Sentiment Analysis task on Financial PhraseBank dataset. Experiments show that this model outperforms the general BERT and other financial domain-specific models.\n","\n","- More details on FinancialBERT's pre-training process can be found at [this article](https://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_Mining):\n","\n","**Training data**\n","\n","- FinancialBERT model was fine-tuned on Financial PhraseBank, a dataset consisting of 4840 Financial News categorised by sentiment (negative, neutral, positive)."],"metadata":{"id":"GPrCE16W5YB7"},"id":"GPrCE16W5YB7"},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import pipeline\n","\n","model = BertForSequenceClassification.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\",num_labels=3)\n","tokenizer = BertTokenizer.from_pretrained(\"ahmedrachid/FinancialBERT-Sentiment-Analysis\")\n","\n","nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n","\n","sentences = [\"Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales.\",\n","             \"Bids or offers include at least 1,000 shares and the value of the shares must correspond to at least EUR 4,000.\",\n","             \"Raute reported a loss per share of EUR 0.86 for the first half of 2009 , against EPS of EUR 0.74 in the corresponding period of 2008.\",\n","             ]\n","results = nlp(sentences)\n","\n","for result in results:\n","    print(f\"Label: {result['label']}, with score: {round(result['score'], 4)}\")"],"metadata":{"id":"MVppTkZz5BLW"},"id":"MVppTkZz5BLW","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. RNN / LSTM (Recurrent Neural Network / Long Short-Term Memory) - Supervised Learning\n"],"metadata":{"id":"Pe5WHQ0Wro7t"},"id":"Pe5WHQ0Wro7t"},{"cell_type":"markdown","source":["<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3ltsv1uzGR6UBjZ6CUs04A.jpeg\" width=500>"],"metadata":{"id":"mwze94Pu9Ptb"},"id":"mwze94Pu9Ptb"},{"cell_type":"markdown","source":["- Commonly used for **supervised learning**\n","\n","- Designed for **sequence data** like **text**, time series, or logs\n","\n","- Remembers context across steps (e.g., previous words or sales days)\n","\n","LSTM is an improved version of RNN that solves the “forgetting” problem"],"metadata":{"id":"y8bMOmBNtMsq"},"id":"y8bMOmBNtMsq"},{"cell_type":"markdown","source":["```python\n","model.add(Embedding(input_dim=5000, output_dim=128))  # Converts word indices into 128-dimensional dense vectors (embedding layer)\n","model.add(LSTM(64))                                   # Adds an LSTM layer with 64 units to capture sequential patterns in the data\n","model.add(Dense(1, activation='sigmoid'))             # Adds an output layer for binary classification (sigmoid activation outputs probability between 0 and 1)\n","\n"],"metadata":{"id":"nV1NWMTQ6S6D"},"id":"nV1NWMTQ6S6D"},{"cell_type":"markdown","source":["🔡 Embedding Layer\n","\n","- Converts each word (as an integer index) into a 128-dimensional vector\n","\n","- input_dim=5000: The model will recognize up to 5,000 unique words\n","\n","- output_dim=128: Each word will be mapped to a 128-length vector\n","\n","🔁 LSTM Layer (Long Short-Term Memory)\n","\n","- Processes the sequence of word embeddings\n","\n","- Remembers context from earlier words (helps with understanding meaning like “not good”)\n","\n","- 64 = number of LSTM cells (units), or how much “memory power” this layer has\n","\n","🧠 Think of this as the \"reader\" of the sentence, remembering important pieces as it goes.\n"],"metadata":{"id":"fvCi4n6tuMC0"},"id":"fvCi4n6tuMC0"},{"cell_type":"markdown","source":["## An Simple Example (Supervised Learning / Supervised Sentiment Analysis)"],"metadata":{"id":"r29QRuq56vvY"},"id":"r29QRuq56vvY"},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Tokenize and pad the text\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Compile the LSTM model and train the model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.optimizers import Adam             # Adam: an efficient optimizer for training\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Set seeds for reproducibility\n","import tensorflow as tf\n","import random\n","seed_value = 42  # Choose any seed value you want\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","tf.random.set_seed(seed_value)"],"metadata":{"id":"Xc6_WofUs3sV"},"id":"Xc6_WofUs3sV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["reviews = [\n","    \"The product quality is excellent and exceeded my expectations!\",\n","    \"Terrible experience. I want a refund.\",\n","    \"Pretty good, but the delivery was slow.\",\n","    \"Absolutely love it! Will buy again.\",\n","    \"The item broke after one week. Very disappointed.\",\n","]\n","\n","labels = [1, 0, 0, 1, 0]  # 1 = positive, 0 = negative\n","\n","df = pd.DataFrame({'Review': reviews, 'Label': labels})\n","\n","df"],"metadata":{"id":"f7zxvVV9s3vJ"},"id":"f7zxvVV9s3vJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tokenize and pad the text"],"metadata":{"id":"6qsBRJ_WySPm"},"id":"6qsBRJ_WySPm"},{"cell_type":"code","source":["# Tokenize and pad the text\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(df['Review'])    # builds a word-to-index dictionary\n","\n","tokenizer.word_index"],"metadata":{"id":"cNpZJ0OlweNE"},"id":"cNpZJ0OlweNE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# turns each sentence into a list of token IDs based on the vocabulary we've built.\n","X = tokenizer.texts_to_sequences(df['Review'])\n","X"],"metadata":{"id":"QpVc9xU-wo1p"},"id":"QpVc9xU-wo1p","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" in deep learning for NLP, especially with models like LSTM or Transformers, the model expects each input to be **a sequence of the same length**. That’s where **padding** comes in.\n","\n"],"metadata":{"id":"b_8axQc1w_tn"},"id":"b_8axQc1w_tn"},{"cell_type":"code","source":["X = pad_sequences(X)   # padding\n","y = df['Label'].values"],"metadata":{"id":"EVukhahawq6O"},"id":"EVukhahawq6O","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# after padding\n","X"],"metadata":{"id":"eBPuwQIQxtFz"},"id":"eBPuwQIQxtFz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Build and compile the LSTM model"],"metadata":{"id":"ls1SaspEyY87"},"id":"ls1SaspEyY87"},{"cell_type":"code","source":["# Initializing the model\n","model = Sequential()\n","model.\n","model.\n","model."],"metadata":{"id":"nZS_B623wq8y"},"id":"nZS_B623wq8y","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuring the model\n","model.compile(optimizer='',\n","              loss='',\n","              metrics=[''])"],"metadata":{"id":"pLJwFZJ2xj0w"},"id":"pLJwFZJ2xj0w","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train the model\n","model.fit( ,  , epochs=15)"],"metadata":{"id":"MTwss6KRwq_d"},"id":"MTwss6KRwq_d","execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss, accuracy = model.evaluate(X, y)\n","print(f\"Accuracy: {accuracy:.2f}\")"],"metadata":{"id":"xDQ-mWvNwrCZ"},"id":"xDQ-mWvNwrCZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Predict probabilities\n","y_probs = model.predict(X)\n","\n","# Step 2: Convert probabilities to class labels (0 or 1)\n","y_pred = (y_probs > 0.5).astype(int)\n","\n","# Step 3: Create confusion matrix\n","cm = confusion_matrix(y, y_pred)\n","cm"],"metadata":{"id":"2bw0joDbzisW"},"id":"2bw0joDbzisW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add results to your DataFrame\n","df['Predicted'] = y_pred\n","df['Confidence'] = y_probs\n","\n","df"],"metadata":{"id":"t-gW6gsmwrFN"},"id":"t-gW6gsmwrFN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Make Predictions"],"metadata":{"id":"A9_3aqOKzSWc"},"id":"A9_3aqOKzSWc"},{"cell_type":"code","source":["test_sentence = [\"Terrible. I want a refund\"]"],"metadata":{"id":"BD8oUhqrwrIH"},"id":"BD8oUhqrwrIH","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize and pad the sentence\n","test_seq = tokenizer.texts_to_sequences(test_sentence)\n","test_pad = pad_sequences(test_seq)\n","\n","# Predict sentiment\n","pred = model.predict(test_pad)[0][0]\n","\n","# Show result\n","sentiment = \"Positive\" if pred > 0.5 else \"Negative\"\n","print(f\"Review: {test_sentence[0]}\")\n","print(f\"Predicted Sentiment: {sentiment} (Confidence: {pred:.2f})\")"],"metadata":{"id":"AF9h5AGv8sWZ"},"id":"AF9h5AGv8sWZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"20ceef4d","metadata":{"id":"20ceef4d"},"source":["---\n","\n","# Discussion Prompts\n","\n","1. How could a business use this sentiment data from customer reviews?\n","2. What might be some challenges with relying solely on sentiment analysis?\n","3. What business decisions could you inform with this insight?\n"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}